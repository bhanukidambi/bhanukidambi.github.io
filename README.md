# Data Engineer
Technical Skills : SQL, Python, Pyspark, Airflow, Scala, GCP, AWS, Databricks, Azure

### Education
Masters of Science in Data Analytics (Apr 2020)

### Work Experience
Employer : Miracle Software Systems (July 2020 - Current)

Role : Sr. Data Engineer

Client : Ford Motor Company (Apr 2025 - Current)
• Contributed to the architecture and evolution of Ford’s Data platform on GCP, delivering batch and streaming pipelines using Airflow, BigQuery, SQL, and Python to power reliable, production-grade data products processing terabytes of dealership data.
• Drove measurable gains in performance and cost efficiency by supporting the migration from Teradata to GCP, redesigning BigQuery tables with partitioning & clustering, and implementing DAG observability to detect anomalies and sudden data spikes before downstream impact.
• Improved delivery speed and platform reliability by translating business requirements into scalable data models and enabling CI/CD and infrastructure

Role : Sr. Software Engineer

Client : The Home Depot (Sept 2024 - Jan 2025)
• Optimized large-scale MySQL and BigQuery environments through advanced SQL tuning, partitioning, and clustering strategies, improving query performance and enabling efficient access to high-volume analytical workloads.
• Enhanced reliability and operational excellence by monitoring Cloud SQL CPU/memory metrics, recommending right-sizing and remediation strategies, implementing automated log sinks for Dataflow failures, and enabling faster incident response.
•Enabled trusted analytics at scale by partnering with analytics and infrastructure teams to support Tableau integrations, troubleshoot cross-system connectivity, and design staging-table methodologies for zero-disruption schema changes.
•Advanced data platform capabilities with AI/ML integrations, building RAG-driven knowledge solutions and modular LLM microservices using Cloud Run, API Gateway, BigQuery, Cloud Storage, and Dataflow for preprocessing and serving.

Role : Cloud Data Engineer

Client : Michigan Central (May 2024 - Sept 2024)
• Contributed to the design and expansion of the GCP data platform, building scalable batch and streaming pipelines with Dataflow (Apache Beam), Pub/Sub, and BigQuery to power reliable, high-volume data ingestion and analytics.
• Improved data accuracy and trust by implementing real-time CDC frameworks, validation layers, and resilient error-handling mechanisms, ensuring consistent data synchronization across distributed systems.
• Expanded platform interoperability through API-based ingestion and outbound data services, enabling secure and efficient data exchange between internal systems and external consumers.
• Strengthened operational maturity by leveraging Cloud Run, Cloud Functions, Airflow, and Cloud Scheduler for orchestration, while performing cost and performance evaluations that guided service selection and architectural decisions.
• Extended the platform with AI capabilities, deploying LLM-powered, enterprise chat solutions integrated with BigQuery and serverless infrastructure to deliver contextual, real-time insights at scale.

Role : Big Data Engineer

Client : Walmart (July 2023 - Apr 2024)
• Built and optimized large-scale ETL pipelines for Walmart’s third-party data platform, processing high-volume datasets from Hive and BigQuery using Spark (Scala/Python) and Airflow-driven orchestration.
• Improved pipeline efficiency and reliability by refining transformation logic, tuning queries, and reducing workflow runtimes while introducing structured data modeling for new application use cases.
• Elevated engineering standards across the platform by implementing CI/CD practices, enforcing data quality frameworks through declarative YAML validations, and maintaining >90% code coverage with Sonar and automated testing.
• Modernized legacy ecosystems by migrating Spark 2.4 workloads to Spark 3.3, transitioning Automic jobs to Airflow, and replacing dependency-heavy scripts with cloud-native gcloud integrations.
• Enabled scalable development and faster onboarding by creating reusable libraries and shared components, leading code reviews, troubleshooting production issues, and ensuring reliable, on-time deployments.
• Maintained strong data governance while handling sensitive customer datasets in compliance with privacy and security requirements.

Role : Data Engineer

client : Ford Motor Company (Sept 2021 - July 2023)
• Built and evolved large-scale data products for connected vehicle analytics, developing end-to-end ETL workflows in PySpark across terabytes of data within distributed Hadoop and GCP environments.
• Improved platform performance and efficiency by redesigning query patterns, optimizing Hive/PySpark interactions, and reducing workflow runtimes from several hours to a fraction of prior execution times.
• Increased data trust and operational visibility by implementing validation frameworks, resolving integrity issues, and launching automated metric and performance alerting for downstream dashboards.
• Supported cloud transformation initiatives by helping migrate workloads from on-prem Hadoop to GCP, running Spark/SQL jobs on Dataproc, orchestrating with Airflow, and contributing to early streaming use cases with Dataflow and Pub/Sub.
• Drove adoption across engineering and analytics teams by incorporating new feature requests, enabling API and external data ingestion, and presenting platform capabilities to expand internal usage.
